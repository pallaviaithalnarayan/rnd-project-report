%!TEX root = ../report.tex
\documentclass[../report.tex]{subfiles}

\begin{document}
    \begin{abstract}
         % Generating semantic segmentation ground truth for training deep learning models is a tidious job. Espececially manual annotations are needed which is time consuming.
         % Synthetical dataset has started overtaking the manual process, wherein the segmentation can be captured using virtual environments using unreal, unity, or blender. 
         % Microsoft have developed an AirSim plugin to be used in virtual environments which supports physics engine for car, drone and computer vision to move around the scene and capture the scene along with segmentation, depth etc. This data generation helps for training deep learning models by varying the environments which in real-time is time consuming. This approach of data generation helps in creating vast amount of data in varieties of dimensions such as height, size, various lighting conditions. 

         This project uses simulated data from AirSim, a tool that integrates with Unreal Engine, to evaluate semantic segmentation. We tackle the issue of limited labeled training data with a two-step method: first, we generate RGB images and semantic ground truth at various altitudes, assigning segmentation IDs to scene objects in the city environments. We implement a deep learning training pipeline that tests different methods with UNet and SegFormer models.
         Our project investigates six scenarios: training from scratch vs. using pre-trained models, performance at different altitudes, combined altitude training, transfer learning between real and synthetic data, and the impact of increasing data size. We validate our approach with the UAVid dataset, illustrating how synthetic data can enhance semantic segmentation in aerial imagery. 

    \end{abstract}
\end{document}
