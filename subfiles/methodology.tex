%!TEX root = ../report.tex
\documentclass[../report.tex]{subfiles}

\begin{document}
    \section{Methodology}
    \label{sec:methodology}

    % Describe all conceptual details about your approach in this section.
    % Add any necessary subsections to improve the presentation.

    % Feel free to rename this section to better reflect the concrete topic you are discussing.

    This section details the methodological approach employed to evaluate the efficacy of semantic segmentation models trained on synthetic data generated using AirSim and Unreal Engine. 
    The evaluation leverages real-world data from the ISPRS dataset and test various hypotheses related to aerial and ground perspectives, data composition, model training strategies, and dataset characteristics. The methodology is organized into the following key components:
    
    1. Synthetic Data Generation, 2. Real-World Dataset Selection, 3. Semantic Segmentation Models.
    
    1. Synthetic Data Generation: 
    For the evaluation framerwork first we need to generate the synthetic semantic segmentation to train the models. 
    a. Tools \& platforms:
    \subfile{tables/methodology_table1.tex}
    Hardware requirements:
    For optimal performance, the following hardware configuration is recommended: 
    GPU: NVIDIA RTX 3090 with 24 GB VRAM
    CPU: Intel Core i9-10900K or AMD Ryzen 9 5900X
    RAM: 128 GB DDR4
    Storage: 2 TB NVMe SSD
    
    Environments:
    Synthetic datasets are created for different aerial heights (10m, 15m, 25m) across various environments, including: City Park, Downtown West, Landscape Mountains Village, Modular Neighborhood package. 

    AirSim setup: 
    By following the airsim documentation https://microsoft.github.io/AirSim/ we need to setup unreal project with airsim, it exposes APIs to interact with vehicle in the simulation programmatically using simulated quadrotor, simulated car or computer vision mode.
    
    Annotations: 
    Each synthetic image is automatically annotated with precise semantic segmentation masks, ensuring accurate ground truth for model training and evaluation with the help of AirSim Image APIs using computer vision mode.

    Data Augmentation:
    Augmentation techniques such as rotation, scaling, and color jittering are applied to enhance dataset variability and robustness.
    
    We generate the datasets by collecting waypoints in the interested unreal environment's scenes. With those 
    As part of R\&D we have a data generation pipeline with complete documentation on how to generate the data in the appendix section and also more details can be explored from airsim documentation https://microsoft.github.io/AirSim/. 

    Once the datasets are generated using our pipeline, we need to choose a real-world dataset for the evaluation of segmentation models. 
    
    2. Real-World Dataset: 
    Aerial real-world datasets are very few in number as manual segmentation of the real-world data is expensive and time consuming. We have choosen ISPRS dataset to compare our results with the real-world data. 

    
    % In this section of the paper the most effective strategies for developing the learning model architecture will be explored. We will also examine the methods and criteria for evaluating its performance, ensuring a comprehensive understanding of both design and assessment processes.

% \begin{figure*}
%   \centering
%   \begin{subfigure}{0.28\linewidth}
%     \includegraphics[width=\textwidth]{figures/binary_threshold.png}
%     \caption{Binary threshold set to 200}
%     \label{fig:binary_threshold}
%   \end{subfigure}
%   \hfill
%   \begin{subfigure}{0.291\linewidth}
%     \includegraphics[width=\textwidth]{figures/set_to_zero.png}
%     \caption{Image set all values below 120 to zero}
%     \label{fig:set_to_zero}
%   \end{subfigure}
%   \hfill
%   \begin{subfigure}{0.305\linewidth}
%     \includegraphics[width=\textwidth]{figures/contrast_stretch.png}
%     \caption{70 \textgreater  Contrast stretch \textless  140}
%     \label{fig:contrast_stretch}
%   \end{subfigure}
%   \caption{Image pre-processing to detect the possible relevant features of a forest ground. Thresholds were selected arbitrarily to enable the clear recognition of particular objects in the image}
%   \label{fig:image_preprocessing}
% \end{figure*}


\subsection{Proposed Approach}

% Currently, the approaches discussed in related work consider locations without GPS access or large-scale images, such as satellite images, which may not change significantly over time. In general, characteristics that remain unchanged in a specific location can be easier to identify in larger images, as they provide more visual information and allow for a better view of surrounding areas. The challenge with focusing on small areas of forest ground is that plants and trees tend to grow over time, making it more difficult to recognize edges or surrounding corners as reference points.

% The proposed network is inspired by the works of Kinari et al. \cite{kinnari_lsvl_2023}, \cite{kinnari_season-invariant_2021} and Samano et al. \cite{samano_global_2021}, integrating a ResNet-based framework with pre-trained weights. It features a convolutional layer head with ReLU activation for effective feature extraction. The network utilizes cosine similarity to compare image outputs. Key considerations in its design include maximizing feature extraction and minimizing computational overhead, ultimately providing a robust framework for visual localization tasks using UAV and satellite imagery, enhancing accuracy and efficiency in various environments.

% The following subsections will address how to overcome these challenges by employing a deep learning-based approach for image matching, utilizing a Convolutional Neural Network (CNN) architecture.

\subsection{Environment Setup}

% The dataset used for this project consist of orthogonal maps created by the Garrulus project from images captures by a drone. Before incorporating the data into a learning-based model, it is essential to identify recognizable objects within the images. This understanding will help define what the model could search for and determine as invariant.

% According to Nixon and Aguado \cite{nixon_feature_2020}, grayscale images are effective for identifying low-level features such as edges and corners. Therefore, a cropped section of an orthogonal map was taken, converted to greyscale, and adjusted for contrast. The pixel values were modified to create a clear image that allows for the detection of specific features depending on each case. The results are illustrated in Figure \ref{fig:image_preprocessing}. As a reference, the pixel value is black when it is zero and white color is represented by the number 255. The binary threshold in  Figure \ref{fig:binary_threshold} sets all pixel values that are 200 or below to zero, while values above 200 are set to 255. In image \ref{fig:set_to_zero}, values from 120 or below are set to zero, while the remaining values retain their greyscale values. Lastly, image \ref{fig:contrast_stretch} enhances contrast by mapping the value 70 to zero and the value 140 to the maximum possible value. Threshold values were selected arbitrarily based on informed judgment and understanding to facilitate the clear identification of specific objects within the image.

% It is important to note that while shadows are prominent in the images, they are not considered invariant features. On the other hand, tree trunks and roots are significant characteristics that are likely to be identified by the model since are the remianing and most remarked objects in the images.


% \subsection{Processing and Classification of Georeferenced Orthoimages}

% The dataset comprises a collection of orthoimages derived from a pair of georeferenced RGB maps, which include localization coordinates with an accuracy of 2.404 centimeters. These orthoimages contain GPS location data. %(EPSG:25832 coordinates) 

% Since labeling the data would have been time-consuming and less beneficial for the model due to the large volume of information contained in each image, the georeferenced maps were divided into smaller images of 224x224 pixels called patches. This size was chosen because it is the only format accepted by the ResNet50 model. Otherwise, the image should be rescaled and lose quality.

% The images were categorized into two groups: positive and negative samples. Positive samples consist of pairs of patches taken from the same area but during different seasons. In contrast, negative samples include patches from different regions of the map that were randomly paired. During this labeling classification, a threshold was applied due to limitations in GPS precision encountered during each drone mission.


\subsection{Data Generation with Groundtruth Segmentation}

% The proposed network draws inspiration from the works of Kinari et al. \cite{kinnari_season-invariant_2021} \cite{kinnari_lsvl_2023} and Samano et al. \cite{samano_global_2021}. This architecture integrates a ResNet-based framework that utilizes pre-trained weights. Unlike previous approaches, it includes a head with convolutional layers and ReLU activation to extract features from the images. The output for each image is then compared using cosine similarity, which provides a measure of similarity between the two vectors, as illustrated in Figure\ref{fig:model_arch}.

% In developing this methodology, several key considerations were taken into account. The architecture is designed to maximize feature extraction while minimizing computational overhead. Leveraging pre-trained weights from established ResNet50 models allows for improved performance on our specific task by utilizing learned features from large datasets.

% This methodology establishes a robust framework for conducting visual localization tasks using \gls{uav} images, enhancing accuracy and efficiency in diverse environmental conditions.

% \begin{figure}[!h]
%   \centering
%   % \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
%    \includegraphics[width=1.0\linewidth]{figures/diagram_II.drawio.png}
%    \caption{The proposed model architecture uses ResNet50, along with a head that includes convolutional layers and ReLU activation functions, to extract features for measuring similarity between vectors.}
%    \label{fig:model_arch}
% \end{figure}

    % SYSTEM OUTLINE
    % Firstly, UAVs capture an image during its flight or a frame extracted from the video as input. Visual localization system preprocesses and describes the input image and the images in the reference image database by feature vectors, then image matching unit measures features similarity. Finally UAVs aerial image is mapped to the large-scale reference image. After matching the UAVs aerial image with the reference image database, the position of the UAVs can be determined according to the best matched reference image; if the matching result does not satisfy the conditions in the reference image database, the UAVs aerial image will be updated to the reference image database.

    % Describe all conceptual details about your approach in this section.
    % Add any necessary subsections to improve the presentation.

    % Feel free to rename this section to better reflect the concrete topic you are discussing.

    % - Introduce your notations such as variable definitions, etc.
     
    %  - Write the building blocks of your methodology as subsections. Indicate them by subsection titles.
     
    %  - For the most essential building block (usually the novel contribution of your thesis), write down the details of the methodology. For example, for a maximum likelihood approach, write down the likelihood equation; for an estimation procedure, write down the estimator.

    %  1. Get ortho map
    %  2. get map patches
    %  3. linear conversion from patches pixels to map coordinates
    %  4. 
    % % Experimental Setup:
    % % - UAV Dataset
    % % - Ground Truth

    
    % Learning rate = 10e-6 as in \cite{kinnari_lsvl_2023}. first it was 0.001

    % NOT SIAMESE NETWORK, BUT:
    % supervised what?? (:


    % NOTE: feature detection through time will be better from a satellite perspective. The current approach might vary A LOT due to the small spaces taken from the dataset
    
\end{document}

